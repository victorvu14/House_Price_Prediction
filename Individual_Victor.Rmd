---
title: "Individual_Assignment"
author: "Victor Vu"
date: "34/05/2019"
output: html_document
---

##Downloading Packages 

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(plyr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(caret)
library(gridExtra)
library(randomForest)
library(ranger)
library(xgboost)
library(data.table)
library(leaflet)
library(DMwR)
```


##Loading Data 
```{r}
train <- read.csv("house_price_train.csv", stringsAsFactors = F)
test <- read.csv("house_price_test.csv", stringsAsFactors = F)
```

#### Data Understanding {.tabset .tabset-fade .tabset-pills}

##### Data Summary
```{r}
summary(train %>%select(-id,-date, -zipcode, -lat, -long))
```
Looking at the data summary, the average house profile has a price tag of 540,000 with 3 bedrooms, 2 bathrooms and a living area of 2000 square feet. The hous is built in 1971 and is in average condition. However, there are definitely outliers that lie way beyond the average profile with the price tag of $7,700,000, having 33 bedrooms, or having a living area of more than 13,000 square feets. 

##### Data Structure

```{r}
str(train)
```
There 17277 observations in the training dataset with 21 variables. Apart from date, all variables are already in numeric form that are ready to be modeled. A thing to note here is that bathrooms and floors are in numeric form. 


##### Check for NAs
The dataset is clean with no NAs 
```{r}
sum(is.na(train))
```

<hr /> 

#### Drop the id and date columns

Since we are not going to use date and id in the analysis, we will go ahead to drop this two columns 

```{r}
train <- train %>%select(-id,-date)
test<- test %>%select(-id,-date)
```

<hr /> 

### Exploratory Data Analysis  {.tabset .tabset-fade .tabset-pills}

#### House Price 

```{r}
ggplot(train, aes(x = price)) +
geom_histogram(col = "black", fill = '#e34a33', binwidth = 200000, center = 100000) +
theme_linedraw() + 
theme(plot.title = element_text(hjust = 0, face = 'bold',color = 'black'), #title settings
      plot.subtitle = element_text(face = "italic")) + #subtitle settings
labs(x = 'Price (USD)', y = 'Frequency', title = "House Sales in King County, USA",
     subtitle = "Price distribution") + #name subtitle
scale_y_continuous(labels = scales::comma, limits = c(0,8000), breaks = c(0,2000,4000,6000,8000)) + 
scale_x_continuous(labels = scales::comma) #prevent scientific number in x-axis
```

The distribution of house price is right skewed with mean larger than median. Most of the houses are from the 200,000 to 600,000 range. 

#### Correlation Analysis

```{r}
corresult = cor(train)

corrplot(corresult, type="upper")

```

We could see from the correlation graph, sqft_living, sqft_living15, sqft_above, grade, and bathrooms are highly correlated. In other words, these variables are explaining the same variations of the data point. Thus we might have to remove and manipulated these variables to avoid multicollinearity. Now lets explore some variables that have high correlation with house price. These include: sqft_living,sqft_above, bedrooms, bathrooms, grade, and location.

#### Price & Sqft_Living

```{r}
train %>% 
 
  ggplot(aes(x=sqft_living,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_living,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Living")+
  ylab("Price")  + scale_x_continuous(labels = scales::comma, breaks = c(0,2000,4000,6000,8000,10000,12000)) +  scale_y_continuous(labels = scales::comma)
```

As we could see there is a positive linear relatition between house price and size of the house. This is sensible as larger house tends to have higher price. But this need to be combine with geolocation in order to have a better view of the picture. 

#### Price & Sqft_Above

Square feet above is the square footage of house apart from basement. This indicates that the house has more than one story. It makes sense that this variable is highly correlated with the Sqft Living, since in most cases, the size of the level above is similar to the basement. 

```{r}
train %>% 
 
  ggplot(aes(x=sqft_above,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_above,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Above")+
  ylab("Price") + 
  ylab("Price")  + scale_x_continuous(labels = scales::comma, breaks = c(0,2000,4000,6000,8000,10000,12000)) +  scale_y_continuous(labels = scales::comma)
```

As we could see, the distribution of this variable is very similar to sqft_living, thus we will go ahead a drop this variable. A more interesting variable to be considered in this case is floor, which indicates the number of floor in the house. 

#### Price & Bedroom 

```{r}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% filter(bedrooms<30)%>%
ggplot(aes(x=bedrooms,y=price,col=bedrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bedrooms vs Price")+ scale_color_gradientn(colors=mycolors)+theme(legend.position="none") +  scale_y_continuous(labels = scales::comma)

```

As we can see, the majority of houses in the data set has 3 to 6 bed rooms. While there appear to be a positive trend, there is no clear correlation between the number of bedroom and house price. 

#### Price & Bathroom

```{r}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% 
ggplot(aes(x=bathrooms,y=price, col=bathrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bathrooms vs Price")+theme(legend.position="none") + scale_y_continuous(labels = scales::comma)
```

There is a clearer positive correlation between the number of bathroom and house price. Highest house price tend to have higher number of bathroom. This might be because house with more bathrooms mean there are more master bedrooms, thus increase the convenience and value of the house. It might be interesting to calculate the ratio of bathroom per bedroom for each house, this will be consider in the feature engineering part.

####Price & Grade 

This is the overall grade given to the housing unit, based on King County grading system. 

```{r}
ggplot(train,aes(grade,log10(price),fill=factor(grade)))+ geom_boxplot(alpha=0.5)+ theme_bw() + 
  labs(x = "Grade", y = "Log Price")+ 
  theme(legend.position="none")
```

As we could observe from the graph, the median house price increases as the grade increases. Higher quality house must have had higher price. 

#### Price & Location 

```{r}
df_map <- train

df_map$PriceBin<-cut(df_map$price, c(0,250e3,500e3,750e3,1e6,2e6,999e6))

center_lon = median(df_map$long,na.rm = TRUE)
center_lat = median(df_map$lat,na.rm = TRUE)

factpal <- colorFactor(c("#2166ac","#67a9cf","#ffffbf","#fee08b","#ef8a62","#b2182b"), 
                       df_map$PriceBin)

leaflet(df_map) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%
  addCircles(lng = ~long, lat = ~lat, 
             color = ~factpal(PriceBin))  %>%
  # controls
  setView(lng=center_lon, lat=center_lat,zoom = 12) %>%
  
  addLegend("bottomright", pal = factpal, values = ~PriceBin,
            title = "House Price Distribution",
            opacity = 1)


```

The map above shows the distribution of the house and the price tag in different bin group. 
House with the red has the highest price and lowest in blue dots. Most of the high price houses locate in Seattle center and in Bellevue or Medina neighborhoods. Houses with lower price are locate toward the South or a little bit further away from the center. This illustration shows that location can be a valuable variable to explain price. 


<hr /> 

### Build Base Model Using Linear Regression 

First we partition the labelled data into train and test set. 

```{r}
f_partition<-function(df,test_proportion=0.2, seed=NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  train_index<-sample(nrow(df), floor(nrow(df)*(1-test_proportion)), replace = FALSE)
  df_train<-df[train_index,]
  df_test<-df[-train_index,]
  
  return(list(train=df_train, test=df_test))
}

whole_data <- f_partition(df = train,
                        test_proportion = 0.2,
                        seed = 123)
```

We adopt Linear Regression as our base model. We apply the algorithm to our train dataset
```{r}
BaseModel = lm(price ~ ., whole_data$train )
summary(BaseModel) 
```

Now we will make prediction using the trained Linear Regression model. This model will be tested on unseen data. We then be able to compare the prediction with the actual value, thus assessing the prediction capability of the model. 

```{r}
BasePred <-predict(BaseModel,whole_data$test)

mape<-function(real,predicted){
  return(mean(abs((real-predicted)/real)))
}
mape(whole_data$test$price,BasePred)
```

Our current model has the R Squared of 0.68. We will go ahead to perform feature engineering to improve the performance of the model. 


###  Feature Importance

```{r}
importance = varImp(BaseModel)
varImportance <- data.frame(Variables = row.names(importance[1]), 
                              Importance = round(importance$Overall,2))
rankImportance <- varImportance %>%
    mutate(Rank = paste0('#',dense_rank(desc(Importance))))

ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                             y = Importance)) +
    geom_bar(stat='identity',colour="white", fill = "#5ab4ac") +
    geom_text(aes(x = Variables, y = 1, label = Rank),
              hjust=0, vjust=.5, size = 4, colour = 'black',
              fontface = 'bold') +
    labs(x = 'Variables', title = 'Relative Variable Importance') +
    coord_flip() + 
    theme_bw()

```
From the table above, we could see the relative importance of the variables. Location, grade, year_built and size of the house are among the most importance variables to explain house price. Other less important variables are sqft_lot15, floors, sqft_above, sqft_living15. However since we will adopt tree base models and these model are not sensitive to collinearity, I will not drop the correlated variables. 

```{r}
train_refined <- train 
```

### Feature Creation 

```{r}
# Create variable that show the ratio of bathroom per bedroom.
train_refined$bath_bed <- train_refined$bathrooms/train_refined$bedrooms

#Create a variable to show the age of the house. We assume that the house is sold in 2015. If the house has been renovated, the age will count from the year of renovation
train_refined$year_old <- 2015 - train_refined$yr_built
for  (row in 1:nrow(train_refined)){
  if (train_refined$yr_renovated[row] > 0){
  train_refined$year_old[row] <- train_refined$year_old[row] - (train_refined$yr_renovated[row] - train_refined$yr_built[row])
}
}
```

Since the house price varies across zipcode however stay constant within the zipcode, thus having a variable to identify the specific neighborhood might help. Also, zipcode is not a meaningful numeric variable by itself, I would like to transform it into categorical variables that the model can understand. I have attempted to add another dataset with specific neighborhood then merge with original by zip code. Then I one hot encode all the neighborhood. However, this for some reason drop the r2 level.
It seems like removing zipcode does help improve the model. 

```{r}
train_refined$zipcode <- NULL
```

### Outlier Treatment {.tabset .tabset-fade .tabset-pills}
We perform outlier detection and treatment for all numeric variables

#### Bedroom

```{r}
ggplot(data = data.frame(train_refined$bedrooms), aes(x = "", y = train_refined$bedrooms)) + geom_boxplot( fill="#e31a1c", alpha=0.5, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bedrooms") + theme_minimal() + theme(axis.title = element_blank())
train_refined <- train_refined %>% filter(bedrooms < 30)
```

There is clearly one significant outlier with 33 bedrooms, we will remove this outlier and only keep data point that is less than 30 rooms.  

#### Bathroom

```{r}
ggplot(data = data.frame(train_refined$bathrooms), aes(x = "", y = train_refined$bathrooms)) + geom_boxplot( fill="#fd8d3c", alpha=0.3, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bathrooms") + theme_minimal() + theme(axis.title = element_blank())
```

There is no significant outlier for Bathroom. 

#### Square Feet Living

```{r}
ggplot(data = data.frame(train_refined$sqft_living), aes(x = "", y = train_refined$sqft_living)) + geom_boxplot( fill="#fed976", alpha=0.3, outlier.colour = 'black', outlier.size = 2) + ggtitle("Boxplot of Square Feet Living") + theme_minimal() + theme(axis.title = element_blank())

train_refined <- train_refined %>% filter(sqft_living < 8000)
```

Most of the house in the train dataset are below 8000. We will go ahead and remove 8 houses above 8000 square meters.

### Standardization & Correcting Skewness
```{r}
#train_refined$sqft_living <- scale(train_refined$sqft_living)
#train_refined$bedrooms <- scale(train_refined$bedrooms)
#train_refined$bathrooms <- scale(train_refined$bathrooms)
#ggplot(data.frame(train_refined$bathrooms), aes(x = train_refined$bathrooms))  + geom_density(fill="#e34a33")

```
I tried to rescale the variable however there is so improvement made to the r2 score. Thus this step will not be taken. 

### Principal Component Analysis {.tabset .tabset-fade .tabset-pills}

Our dataset currently has 17 independent variables, which is partly correlated. Thus we can reduce the collinearity by applying principal component analysis. Below layout my experiment with PCA.

#### Rotations
```{r}
pca <- prcomp(train_refined%>%select(-price), scale = TRUE, center=TRUE)
pca_var <- pca$sdev^2
pca_var_round <- round(pca_var/sum(pca_var)*100, 1)
print(pca$rotation[,1:6])
```
#### Screenplot
```{r}
cumsum(pca_var_round)
barplot(pca_var_round, main = " Screen Plot", xlab ="Principal Component", ylab = "Percent Variation")
```

#### L Plot
```{r}
plot(pca, type = "l")
```

As we can see the tenth first component explains almost 96% of the variation. So we will go ahead and experiment build a basic linear model with 10 components. 

#### Basic Linear Model
```{r}
new_vars<-pca$x  # we first take all of them
new_vars_1<-data.table(price = train_refined$price,new_vars)
lm1<-lm(price~., data=new_vars_1[, c(1:11)])
summary(lm1)
```

As I applied the newly created components to the random forest model, I got worse results. Thus I will not proceed with the components created. Now I will go ahead with experimenting different algorithms and parameter tuning. 


## Building Final Model 
First partition the data into train and test sets
```{r}
final_data <- f_partition(df = train_refined,
                        test_proportion = 0.2,
                        seed = 123)
```

### Random Forest {.tabset .tabset-fade .tabset-pills}

#### Ranger Model with Tuning
```{r echo=FALSE, message=FALSE, warning=FALSE}
#Define target variable
formula<-as.formula(price~.)

#Define tuneGrid
tuneGrid=data.table(expand.grid(mtry=c(5,10),
                                splitrule='variance',
                                min.node.size=c(2,5,10)))

fit <- train(
    formula
    ,data = final_data$train
    ,method = "ranger"
    ,trControl = trainControl(method="cv", number = 5, search ="grid", allowParallel = TRUE, verbose = TRUE)
    ,tuneGrid=tuneGrid
    ,importance = 'impurity'
)

rf_1<-ranger(formula, final_data$train, num.trees=1000,
                   mtry=fit$bestTune$mtry,
                   min.node.size=fit$bestTune$min.node.size)
test_rf1<-predict(rf_1, final_data$test)$predictions
mape(real=final_data$test$price, predicted = test_rf1)

```

The current MAPE is 0.13. This is a big improvement from the base model. This could be because the random forest pick up better all the non linearity pattern in the data. Also, dropping of outliers and feature creations show added value here. 

#### Variable Importance
```{r}
plot(varImp(fit))
```

It seems like the yr_renovated, floors, bedrooms, and condition have very low importance in the model. 

#### Plotting Prediction
```{r}
df_pred<-data.table(id=1:length(test_rf1),
                    real=final_data$test$price,
                    test_rf1)
ggplot(df_pred,aes(x=test_rf1,y=real))+geom_point(color="#5ab4ac")+geom_abline(color="#e34a33") +scale_x_continuous(labels = scales::comma) +  scale_y_continuous(labels = scales::comma)
```

### XGBoost Model {.tabset .tabset-fade .tabset-pills}

#### Setting Parameters Level & Cross Validate
```{r}
#Putting dataset in matrix format
dtrain <- xgb.DMatrix(data = as.matrix(final_data$train[,-1]),label = final_data$train$price)
dtest <- xgb.DMatrix(data = as.matrix(final_data$test[,-1]),label = final_data$test$price)

#Defining parameters
params <- list(booster = "gbtree", objective = "reg:linear", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

#Cross validate by 5 folds 
xgbcv <- xgb.cv( params = params, data=dtrain, nrounds = 500, nfold = 5, showsd = T, stratified = T, print_every_n = 50, early_stopping_rounds = 20, maximize = F)

#Find the best iteration
xgbcv$best_iteration
```

#### RSquared
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

xgb1 <- xgb.train (params = params, data = dtrain, nrounds = xgbcv$best_iteration, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "rmse")
xgbpred <- predict (xgb1,dtest)
mape(real=final_data$test$price, predicted = xgbpred)
```
#### Variable Importance 

```{r}
xgb.plot.importance(xgb.importance(model = xgb1))
```

#### Plotting Prediction
```{r}
xgb_pred<-data.table(id=1:length(test_xgb),
                    real=final_data$test$price,
                    test_xgb)
ggplot(xgb_pred,aes(x=test_xgb,y=real))+geom_point(color="#5ab4ac")+geom_abline(color="#e34a33") +  scale_x_continuous(labels = scales::comma) +  scale_y_continuous(labels = scales::comma)
```

### Tuning XGBoost Model
```{r}

library(mlr)

lrn <- makeLearner("regr.xgboost")

#set parameter space
params <- makeParamSet(
  makeIntegerParam("nrounds",lower=10,upper=20),
  makeIntegerParam("max_depth",lower=1,upper=5),
  makeNumericParam("lambda",lower=0.55,upper=0.60),
  makeNumericParam("eta", lower = 0.001, upper = 0.5),
  makeNumericParam("subsample", lower = 0.10, upper = 0.80),
  makeNumericParam("min_child_weight",lower=1,upper=5),
  makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8)
)
#set resampling strategy
rdesc <- makeResampleDesc("CV",iters=5L)

#Search startegy
ctrl <- makeTuneControlRandom(maxit = 10L)

#Set parallel backend
library(parallel)
library(parallelMap) 
parallelStartSocket(cpus = detectCores())

traintask <- makeRegrTask(data = final_data$train, target = "price")
testTask <- makeRegrTask(data = final_data$test, target = "price")

#Define tuning functions
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, par.set = params, control = ctrl,show.info = FALSE)

mytune
```
#### Apply optimal parameters to model
```{r}
xgb_pars <- setHyperPars(learner = lrn,
                        par.vals = mytune$x)

xgb_tuned <- train(learner = lrn,task = traintask)
```

#### Apply Model on Test set
```{r}
xgbtuned_pred <- predict(xgb_tuned, testTask)
mape(real=xgbtuned_pred$data$truth, predicted = xgbtuned_pred$data$response)
```

The result of the tuned XGBoost model is slightly less than the tuned Random Forrest. Thus we will use the tuned XGBoost to build our final model. 


