---
title: "House Price Prediction"
author: "Victor Vu"
date: "26/05/2019"
output: html_document
---
### Github Repository of Project

https://github.com/victorvu14/House_Price_Prediction

### Scope 

For this project, we were given the dataset from Kaggle https://www.kaggle.com/marklvl/bike-sharing-dataset/home containing information about the houses in King County. The dataset contains 19 house features plus the price and the id columns, along with 21613 observations. The aim of the project is to build a regression model to predict house price with minimal MAPE. 

The following variables are included in the data:

  1. ida:  notation for a house
  2. date: Date house was sold
  3. price: Price is prediction target
  4. bedrooms: Number of Bedrooms/House
  5. bathrooms: Number of bathrooms/House
  6. sqft_living: square footage of the home
  7. sqft_lot: square footage of the lot
  8. floors: Total floors (levels) in house
  9. waterfront: House which has a view to a waterfront
  10. view: Has been viewed
  11. condition: How good the condition is ( Overall )
  12. grade: overall grade given to the housing unit, based on King County grading system
  13. sqft_above: square footage of house apart from basement
  14. sqft_basement: square footage of the basement
  15. yr_built: Built Year
  16. yr_renovated: Year when house was renovated
  17. zipcode: zip
  18. lat: Latitude coordinate
  19. long: Longitude coordinate
  20. sqft_living15: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area
  21. sqft_lot15: lotSize area in 2015(implies-- some renovations)
  
<hr /> 

#### Downloading Packages 

```{r include=FALSE}
if(!"data.table" %in% installed.packages()) {
  install.packages("data.table")
} else {
  print('data.table package already installed')}
library(data.table)

if(!"knitr" %in% installed.packages()) {
  install.packages("knitr")
} else {
  print('knitr package already installed')}
library(knitr)

if(!"ggplot2" %in% installed.packages()) {
  install.packages("ggplot2")
} else {
  print('ggplot2 package already installed')}
library(ggplot2)

if(!"plyr" %in% installed.packages()) {
  install.packages("plyr")
} else {
  print('plyr package already installed')}
library(plyr)

if(!"dplyr" %in% installed.packages()) {
  install.packages("dplyr")
} else {
  print('dplyr package already installed')}
library(dplyr)

if(!"corrplot" %in% installed.packages()) {
  install.packages("corrplot")
} else {
  print('corrplot package already installed')}
library(corrplot)

if(!"RColorBrewer" %in% installed.packages()) {
  install.packages("RColorBrewer")
} else {
  print('RColorBrewer package already installed')}
library(RColorBrewer)

if(!"caret" %in% installed.packages()) {
  install.packages("caret")
} else {
  print('caret package already installed')}
library(caret)

if(!"gridExtra" %in% installed.packages()) {
  install.packages("gridExtra")
} else {
  print('gridExtra package already installed')}
library(gridExtra)

if(!"ranger" %in% installed.packages()) {
  install.packages("ranger")
} else {
  print('ranger package already installed')}
library(ranger)

if(!"xgboost" %in% installed.packages()) {
  install.packages("xgboost")
} else {
  print('xgboost package already installed')}
library(xgboost)

if(!"leaflet" %in% installed.packages()) {
  install.packages("leaflet")
} else {
  print('leaflet package already installed')}
library(leaflet)

if(!"DMwR" %in% installed.packages()) {
  install.packages("DMwR")
} else {
  print('DMwR package already installed')}
library(DMwR)

if(!"mlr" %in% installed.packages()) {
  install.packages("mlr")
} else {
  print('mlr package already installed')}
library(mlr)

if(!"parallel" %in% installed.packages()) {
  install.packages("parallel")
} else {
  print('parallel package already installed')}
library(parallel)

if(!"parallelMap" %in% installed.packages()) {
  install.packages("parallelMap")
} else {
  print('parallelMap package already installed')}
library(parallelMap) 
```

<hr /> 

#### Loading Data 
```{r}
train <- read.csv("house_price_train.csv", stringsAsFactors = F)
test <- read.csv("house_price_test.csv", stringsAsFactors = F)
```

<hr /> 

#### Data Understanding {.tabset .tabset-fade .tabset-pills}

##### Data Summary

Looking at the data summary, the average house profile has a price tag of 540,000 with 3 bedrooms, 2 bathrooms and a living area of 2000 square feet. The hous is built in 1971 and is in average condition. However, there are definitely outliers that lie way beyond the average profile with the price tag of $7,700,000, having 33 bedrooms, or having a living area of more than 13,000 square feets. 

```{r}
summary(train %>%select(-id,-date, -zipcode, -lat, -long))
```

##### Data Structure

There 17277 observations in the training dataset with 21 variables. Apart from date, all variables are already in numeric form that are ready to be modeled. A thing to note here is that bathrooms and floors are in numeric form. 

```{r}
str(train)
```


##### Check for NAs

The dataset is clean with no NAs 

```{r}
sum(is.na(train))
```

<hr /> 

#### Drop the id and date columns

Since we are not going to use date and id in the analysis, we will go ahead to drop this two columns 

```{r}
train <- train %>%select(-id,-date)

#Save the id for submission 
id <- data.frame(test$id)
test<- test %>%select(-id,-date)
```

<hr /> 

### Exploratory Data Analysis  {.tabset .tabset-fade .tabset-pills}

#### House Price 

The distribution of house price is right skewed with mean larger than median. Most of the houses are from the 200,000 to 600,000 range. We will later need to rescale for modeling purpose.

```{r echo=FALSE}
ggplot(train, aes(x = price)) +
geom_histogram(col = "black", fill = '#e34a33', binwidth = 200000, center = 100000) +
theme_linedraw() + 
theme(plot.title = element_text(hjust = 0, face = 'bold',color = 'black'), #title settings
      plot.subtitle = element_text(face = "italic")) + #subtitle settings
labs(x = 'Price (USD)', y = 'Frequency', title = "House Sales in King County, USA",
     subtitle = "Price distribution") + #name subtitle
scale_y_continuous(labels = scales::comma, limits = c(0,8000), breaks = c(0,2000,4000,6000,8000)) + 
scale_x_continuous(labels = scales::comma) #prevent scientific number in x-axis
```

#### Correlation Analysis

We could see from the correlation graph, sqft_living, sqft_living15, sqft_above, grade, and bathrooms are highly correlated. In other words, these variables are explaining the same variations of the data point. Thus we might have to remove and manipulated these variables to avoid multicollinearity. Now lets explore some variables that have high correlation with house price. These include: sqft_living,sqft_above, bedrooms, bathrooms, grade, and location.

```{r echo=FALSE}
corresult = cor(train)

corrplot(corresult, type="upper")

```

#### Price & Sqft_Living

As we could see there is a positive linear relatition between house price and size of the house. This is sensible as larger house tends to have higher price. But this need to be combine with geolocation in order to have a better view of the picture. 

```{r echo=FALSE}
train %>% 
 
  ggplot(aes(x=sqft_living,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_living,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Living")+
  ylab("Price")  + scale_x_continuous(labels = scales::comma, breaks = c(0,2000,4000,6000,8000,10000,12000)) +  scale_y_continuous(labels = scales::comma)
```

#### Price & Sqft_Above

Square feet above is the square footage of house apart from basement. This indicates that the house has more than one story. It makes sense that this variable is highly correlated with the Sqft Living, since in most cases, the size of the level above is similar to the basement. 
As we could see, the distribution of this variable is very similar to sqft_living, thus we will go ahead a drop this variable. A more interesting variable to be considered in this case is floor, which indicates the number of floor in the house.

```{r echo=FALSE}
train %>% 
 
  ggplot(aes(x=sqft_above,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_above,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Above")+
  ylab("Price") + 
  ylab("Price")  + scale_x_continuous(labels = scales::comma, breaks = c(0,2000,4000,6000,8000,10000,12000)) +  scale_y_continuous(labels = scales::comma)
```

#### Price & Bedroom 

As we can see, the majority of houses in the data set has 3 to 6 bed rooms. While there appear to be a positive trend, there is no clear correlation between the number of bedroom and house price. 

```{r echo=FALSE}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% filter(bedrooms<30)%>%
ggplot(aes(x=bedrooms,y=price,col=bedrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bedrooms vs Price")+ scale_color_gradientn(colors=mycolors)+theme(legend.position="none") +  scale_y_continuous(labels = scales::comma)

```

#### Price & Bathroom

There is a clearer positive correlation between the number of bathroom and house price. Highest house price tend to have higher number of bathroom. This might be because house with more bathrooms mean there are more master bedrooms, thus increase the convenience and value of the house. It might be interesting to calculate the ratio of bathroom per bedroom for each house, this will be consider in the feature engineering part.

```{r echo=FALSE}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% 
ggplot(aes(x=bathrooms,y=price, col=bathrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bathrooms vs Price")+theme(legend.position="none") + scale_y_continuous(labels = scales::comma)
```

####Price & Grade 

This is the overall grade given to the housing unit, based on King County grading system. As we could observe from the graph, the median house price increases as the grade increases. Higher quality house must have had higher price.  

```{r echo=FALSE}
ggplot(train,aes(grade,log10(price),fill=factor(grade)))+ geom_boxplot(alpha=0.5)+ theme_bw() + 
  labs(x = "Grade", y = "Log Price")+ 
  theme(legend.position="none")
```

#### Price & Location 

The map below shows the distribution of the house and the price tag in different bin group. 
House with the red has the highest price and lowest in blue dots. Most of the high price houses locate in Seattle center and in Bellevue or Medina neighborhoods. Houses with lower price are locate toward the South or a little bit further away from the center. This illustration shows that location can be a valuable variable to explain price. 


```{r echo=FALSE}
df_map <- train

df_map$PriceBin<-cut(df_map$price, c(0,250e3,500e3,750e3,1e6,2e6,999e6))

center_lon = median(df_map$long,na.rm = TRUE)
center_lat = median(df_map$lat,na.rm = TRUE)

factpal <- colorFactor(c("#2166ac","#67a9cf","#ffffbf","#fee08b","#ef8a62","#b2182b"), 
                       df_map$PriceBin)

leaflet(df_map) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%
  addCircles(lng = ~long, lat = ~lat, 
             color = ~factpal(PriceBin))  %>%
  # controls
  setView(lng=center_lon, lat=center_lat,zoom = 12) %>%
  
  addLegend("bottomright", pal = factpal, values = ~PriceBin,
            title = "House Price Distribution",
            opacity = 1)


```

<hr /> 

### Build Base Model {.tabset .tabset-fade .tabset-pills}

#### Data Partitioning

First we partition the labelled data into train and test set. We will spit the train dataset into 80/20.

```{r}
f_partition<-function(df,test_proportion=0.2, seed=NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  train_index<-sample(nrow(df), floor(nrow(df)*(1-test_proportion)), replace = FALSE)
  df_train<-df[train_index,]
  df_test<-df[-train_index,]
  
  return(list(train=df_train, test=df_test))
}

whole_data <- f_partition(df = train,
                        test_proportion = 0.2,
                        seed = 123)
```

#### Model Summary

We adopt Linear Regression as our base model. We apply the algorithm to our train dataset

```{r}
BaseModel = lm(price ~ ., whole_data$train )
summary(BaseModel) 
```

#### Prediction

Now we will make prediction using the trained Linear Regression model. This model will be tested on unseen data. We then be able to compare the prediction with the actual value, thus assessing the prediction capability of the model. 

```{r warning=FALSE}
BasePred <-predict(BaseModel,whole_data$test)

mape<-function(real,predicted){
  return(mean(abs((real-predicted)/real)))
}
mape(whole_data$test$price,BasePred)
```

Our current model has the MAPE value of of 0.257. We will go ahead to perform feature engineering to improve the performance of the model. 


###  Feature Importance

```{r}
importance = varImp(BaseModel)
varImportance <- data.frame(Variables = row.names(importance[1]), 
                              Importance = round(importance$Overall,2))
rankImportance <- varImportance %>%
    mutate(Rank = paste0('#',dense_rank(desc(Importance))))

ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                             y = Importance)) +
    geom_bar(stat='identity',colour="white", fill = "#5ab4ac") +
    geom_text(aes(x = Variables, y = 1, label = Rank),
              hjust=0, vjust=.5, size = 4, colour = 'black',
              fontface = 'bold') +
    labs(x = 'Variables', title = 'Relative Variable Importance') +
    coord_flip() + 
    theme_bw()

```

From the table above, we could see the relative importance of the variables. Location, grade, year_built and size of the house are among the most importance variables to explain house price. Other less important variables are sqft_lot15, floors, sqft_above, sqft_living15. However since we will adopt tree base models and these model are not sensitive to collinearity, I will not drop the correlated variables. 

```{r include=FALSE}
train_refined <- train 
```

<hr /> 

### Feature Creation 

Now I will go ahead to calculate the ration of bathroom per bedroom. Since this might indicate the convinience of the house. My hypothesis is that house with higher level might have a higher price to comparable house. The second variable I will create is year old. We assume that the house is sold in 2015. If the house has been renovated, the age will count from the year of renovation.

```{r}
# Create variable that show the ratio of bathroom per bedroom.
train_refined$bath_bed <- train_refined$bathrooms/train_refined$bedrooms
test$bath_bed <- test$bathrooms/test$bedrooms

#Create a variable to show the age of the house. 
train_refined$year_old <- 2015 - train_refined$yr_built
for  (row in 1:nrow(train_refined)){
  if (train_refined$yr_renovated[row] > 0){
  train_refined$year_old[row] <- train_refined$year_old[row] - (train_refined$yr_renovated[row] - train_refined$yr_built[row])
}
}

#Create similar variable for the test set
test$year_old <- 2015 - test$yr_built
for  (row in 1:nrow(test)){
  if (test$yr_renovated[row] > 0){
  test$year_old[row] <- test$year_old[row] - (test$yr_renovated[row] - test$yr_built[row])
}
}
```

Since the house price varies across zipcode however stay constant within the zipcode, thus having a variable to identify the specific neighborhood might help. Also, zipcode is not a meaningful numeric variable by itself, I would like to transform it into categorical variables that the model can understand. I have attempted to add another dataset with specific neighborhood then merge with original by zip code. Then I one hot encode all the neighborhood. However, this for some reason drop the r2 level.
It seems like removing zipcode does help improve the model. 

```{r}
train_refined$zipcode <- NULL
test$zipcode <- NULL
```

<hr /> 

### Outlier Treatment {.tabset .tabset-fade .tabset-pills}
We perform outlier detection and treatment for all numeric variables

#### Bedroom

There is clearly one significant outlier with 33 bedrooms, we will remove this outlier and only keep data point that is less than 30 rooms. 

```{r echo=FALSE}
ggplot(data = data.frame(train_refined$bedrooms), aes(x = "", y = train_refined$bedrooms)) + geom_boxplot( fill="#e31a1c", alpha=0.5, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bedrooms") + theme_minimal() + theme(axis.title = element_blank())
train_refined <- train_refined %>% filter(bedrooms < 30)
```

#### Bathroom

There is no significant outlier for Bathroom. 

```{r echo=FALSE}
ggplot(data = data.frame(train_refined$bathrooms), aes(x = "", y = train_refined$bathrooms)) + geom_boxplot( fill="#fd8d3c", alpha=0.3, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bathrooms") + theme_minimal() + theme(axis.title = element_blank())
```

There is no significant outlier for Bathroom. 

#### Square Feet Living

Most of the house in the train dataset are below 8000. We will go ahead and remove 8 houses above 8000 square meters.

```{r echo=FALSE}
ggplot(data = data.frame(train_refined$sqft_living), aes(x = "", y = train_refined$sqft_living)) + geom_boxplot( fill="#fed976", alpha=0.3, outlier.colour = 'black', outlier.size = 2) + ggtitle("Boxplot of Square Feet Living") + theme_minimal() + theme(axis.title = element_blank())

train_refined <- train_refined %>% filter(sqft_living < 8000)
```

<hr /> 

### Standardization & Correcting Skewness

We now need to detect skewness in the Target value. Let's see what is the effect of skewness on a variable, and plot it using ggplot. The way of getting rid of the skewness is to use the log (or the log1p) of the values of that feature, to flatten it. To reduce right skewness, take roots or logarithms or reciprocals (x to 1/x). This is the commonest problem in practice. To reduce left skewness, take squares or cubes or higher powers.

```{r echo=FALSE}
df <- rbind(data.frame(version="price",x=train_refined$price),
            data.frame(version="log(price+1)",x=log(train_refined$price + 1)))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

We therefore transform the target value applying log

```{r}
# Log transform the target for official scoring
dataset <- train_refined
dataset$price <- log1p(dataset$price)
```

The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation

I will set up my threshold for the skewness in 0.75. I place that value in that variable to adjust its value in a single place, in case I have to perform multiple tests.

```{r}
skewness_threshold = 0.75
```

Now, let's compute the skewness of each feature that is not 'factor' nor 'character'. So, I'm only interested in continuous values. One possible way of doing it is the following: First, lets determine what is the 'class' or data type of each of my features.

```{r}
column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)
numeric_columns <- names(column_types[column_types != "factor"])
```

Now we apply fix skewness to all numeric variables

```{r}
# skew of each variable
skew <- sapply(numeric_columns, function(x) { 
    e1071::skewness(dataset[[x]], na.rm = T)
  }
)

# Apply to real test set
skew1 <- sapply(numeric_columns[-1], function(x) { 
    e1071::skewness(test[[x]], na.rm = T)
  }
)
# Since there are negative values in the longtitude, we will add the value to the minimum of the longtitude
dataset$long <- dataset$long - min(dataset$long)
test$long <- test$long - min(test$long)

# transform all variables above a threshold skewness.
skew <- skew[abs(skew) > skewness_threshold]

for(x in names(skew)) {
  dataset[[x]] <- log(dataset[[x]] + 1)
}

# apply skewness transformation for test set 
for(x in names(skew)) {
  test[[x]] <- log(test[[x]] + 1)
}

```

<hr /> 

### Principal Component Analysis {.tabset .tabset-fade .tabset-pills}

Our dataset currently has 19 independent variables, which is partly correlated. Thus we can reduce the collinearity by applying principal component analysis. Below layout my experiment with PCA.

#### Screenplot
```{r}
pca <- prcomp(dataset%>%select(-price), scale = TRUE, center=TRUE)
pca_var <- pca$sdev^2
pca_var_round <- round(pca_var/sum(pca_var)*100, 1)

cumsum(pca_var_round)
barplot(pca_var_round, main = " Screen Plot", xlab ="Principal Component", ylab = "Percent Variation")
```

#### L Plot

As we can see the tenth first component explains almost 91% of the variation. So we will go ahead and experiment build a basic linear model with 10 components. 

```{r}
plot(pca, type = "l")
```

#### Basic Linear Model

This model has a train R Squared of 0.63, which is slightly lower than the previous base model. As I applied the newly created components to the random forest model, I got worse results. Thus I will not proceed with the components created. Now I will go ahead with experimenting different algorithms and parameter tuning. 

```{r echo=FALSE}
new_vars<-pca$x  # we first take all of them
new_vars_1<-data.table(price = train_refined$price,new_vars)
lm1<-lm(price~., data=new_vars_1[, c(1:11)])
summary(lm1)
```

<hr /> 

## Building Final Model 
First partition the data into train and test sets
```{r}
final_data <- f_partition(df = dataset,
                        test_proportion = 0.2,
                        seed = 123)
```

### Random Forest {.tabset .tabset-fade .tabset-pills}

#### Ranger Model

The current MAPE is 0.125. This is a big improvement from the base model. This could be because the random forest pick up better all the non linearity pattern in the data. Also, dropping of outliers, fix skewness and feature creations show added value here. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
#Define target variable
formula<-as.formula(price~.)

rf_1<-ranger(formula, final_data$train, num.trees=1000,
                   mtry=10,
                   min.node.size=2)
                     
test_rf1 <- as.numeric(exp(predict(rf_1, final_data$test)$predictions)-1)
mape(real=as.numeric(exp(final_data$test$price)-1),predicted = test_rf1)
```

#### Plotting Prediction
```{r echo=FALSE}
df_pred<-data.table(id=1:length(test_rf1),
                    real=as.numeric(exp(final_data$test$price)-1),
                    test_rf1)
ggplot(df_pred,aes(x=test_rf1,y=real))+geom_point(color="#5ab4ac")+geom_abline(color="#e34a33") +scale_x_continuous(labels = scales::comma) +  scale_y_continuous(labels = scales::comma)
```

<hr /> 

### XGBoost Model {.tabset .tabset-fade .tabset-pills}

#### Tuning XGBoost Model

```{r message=FALSE, warning=FALSE}

lrn <- makeLearner("regr.xgboost")

#set parameter space
params <- makeParamSet(
  makeIntegerParam("nrounds",lower=10,upper=250),
  makeIntegerParam("max_depth",lower=1,upper=20),
  makeNumericParam("lambda",lower=0.1,upper=0.6),
  makeNumericParam("eta", lower = 0.001, upper = 0.5),
  makeNumericParam("subsample", lower = 0.10, upper = 0.80),
  makeNumericParam("min_child_weight",lower=1,upper=5),
  makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8)
)
#set resampling strategy
rdesc <- makeResampleDesc("CV",iters=5L)

#Search startegy
ctrl <- makeTuneControlRandom(maxit = 10L)

#Set parallel backend
parallelStartSocket(8)

traintask <- makeRegrTask(data = final_data$train, target = "price")
testTask <- makeRegrTask(data = final_data$test, target = "price")

#Define tuning functions
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, par.set = params, control = ctrl,show.info = FALSE)
mytune
```

#### Applying Parameters Level & Cross Validate

```{r}
#Putting dataset in matrix format
dtrain <- xgb.DMatrix(data = as.matrix(final_data$train[,-1]),label = final_data$train$price)
dtest <- xgb.DMatrix(data = as.matrix(final_data$test[,-1]),label = final_data$test$price)

#Cross validate by 5 folds 
xgbcv <- xgb.cv( params = mytune, data=dtrain, nrounds = 1000, nfold = 5, showsd = T, stratified = T, print_every_n = 50, early_stopping_rounds = 20, maximize = F)

#Find the best iteration
xgbcv$best_iteration
```

#### MAPE

The MAPE result of the tuned XGBoost model is 0.124, which is slightly less than the tuned Random Forrest. Thus we will use the tuned XGBoost to build our final model.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}

xgb1 <- xgb.train (params = mytune, data = dtrain, nrounds = 100, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "mae")
xgbpred <- as.numeric(exp(predict(xgb1,dtest))-1)

```

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
mape(real=as.numeric(exp(final_data$test$price)-1), predicted = xgbpred)
```

#### Variable Importance 

We could see that bedrooms, bathrooms, yr_renovated and floors have low importance in this model. Thus we will experiment dropping these variables to see if our performance improve. I have attempted to build another model with these variables dropped, however the MAPE increase, thus, I will keep these variables. 

```{r echo=FALSE}
xgb.plot.importance(xgb.importance(model = xgb1))
```

#### Plotting Prediction

```{r echo=FALSE}
xgb_table<-data.table(id=1:length(xgbpred),
                    real=as.numeric(exp(final_data$test$price)-1),
                    xgbpred)
ggplot(xgb_table,aes(x=xgbpred,y=real))+geom_point(color="#5ab4ac")+geom_abline(color="#e34a33") +  scale_x_continuous(labels = scales::comma) +  scale_y_continuous(labels = scales::comma)

```

<hr /> 

### Final Prediction - XGBoost {.tabset .tabset-fade .tabset-pills}

#### Tuning on Whole Trainset

```{r echo=TRUE, message=FALSE, warning=FALSE}
parallelStartSocket(8)
### Trainset
train_final <- makeRegrTask(data = dataset, target = "price")

#Define tuning functions
tune_final <- tuneParams(learner = lrn, task = train_final, resampling = rdesc, par.set = params, control = ctrl,show.info = FALSE)
tune_final
```

#### Making Predictions

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
#Create variable price in the test set
test$price <- NA
train_df <- xgb.DMatrix(data = as.matrix(dataset[,-1]),label = dataset$price)
test_df <- xgb.DMatrix(data = as.matrix(test[,-20]), label = test$price)

xgb_final <- xgb.train(params = tune_final, data = train_df, nrounds = 154 , watchlist = list(val=test_df,train=train_df), print_every_n = 100, early_stopping_rounds = 10, maximize = F , eval_metric = "mae")

final_prediction <- as.numeric(exp(predict(xgb_final,test_df))-1)
#Add the final prediction to the id dataframe
id$SalePrice <- final_prediction

#Exporting prediction

#write.csv(id, file = "submission.csv", row.names = FALSE) 
```

### Conclusion

House prices in King County are heavily right skewed where most house ranges from 200,000 to 600,000. Most houses are smaller than 2000 squared feet with less than 3 bedrooms and 2 bathrooms. Given the task of predicting house price, I have used Linear Regression to build the base model yielding MAPE of 0.25 . To improved the performance of the model several steps were taken such as Feature Creation, Outlier Removal and Correcting Skewness. All these steps were feed into more robust algorithm of Random Forest and XGBoost. The best model is XGBoost with a MAPE of 0.12. I have also attempted to use PCA however this step does not help. Finally, I have performed grid search to best tuning the model. 


