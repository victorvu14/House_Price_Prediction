---
title: "Individual_Assignment"
author: "Victor Vu"
date: "34/05/2019"
output: html_document
---

##Downloading Packages 

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(plyr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(caret)
library(gridExtra)
library(randomForest)
library(xgboost)
library(data.table)
library(leaflet)
library(DMwR)
```


##Loading Data 
```{r}
train <- read.csv("house_price_train.csv", stringsAsFactors = F)
test <- read.csv("house_price_test.csv", stringsAsFactors = F)
```

#### Data Understanding {.tabset .tabset-fade .tabset-pills}

##### Data Summary
```{r}
summary(train %>%select(-id,-date, -zipcode, -lat, -long))
```
Looking at the data summary, the average house profile has a price tag of 540,000 with 3 bedrooms, 2 bathrooms and a living area of 2000 square feet. The hous is built in 1971 and is in average condition. However, there are definitely outliers that lie way beyond the average profile with the price tag of $7,700,000, having 33 bedrooms, or having a living area of more than 13,000 square feets. 

##### Data Structure

```{r}
str(train)
```
There 17277 observations in the training dataset with 21 variables. Apart from date, all variables are already in numeric form that are ready to be modeled. A thing to note here is that bathrooms and floors are in numeric form. 


##### Check for NAs
The dataset is clean with no NAs 
```{r}
sum(is.na(train))
```

<hr /> 

#### Drop the id and date columns

Since we are not going to use date and id in the analysis, we will go ahead to drop this two columns 

```{r}
train <- train %>%select(-id,-date)
```

<hr /> 

### Exploratory Data Analysis  {.tabset .tabset-fade .tabset-pills}

Since the distribution of house price is right skewed, we applied log of house price to plot the distribution. The log transformation can be used to make highly skewed distributions less skewed.This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.

```{r}
ggplot(train,aes(x = log10(price)))+ geom_histogram(fill="#e34a33",binwidth=0.10)
```



#### Price & Sqft_Living

```{r}
train %>% 
 
  ggplot(aes(x=sqft_living,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_living,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Living")+
  ylab("Price")
```

As we could see there is a positive linear relatition between house price and size of the house. This is sensible as larger house tends to have higher price. But this need to be combine with geolocation in order to have a better view of the picture. 

#### Price & Sqft_Above

Square feet above is the square footage of house apart from basement. This indicates that the house has more than one story. It makes sense that this variable is highly correlated with the Sqft Living, since in most cases, the size of the level above is similar to the basement. 

```{r}
train %>% 
 
  ggplot(aes(x=sqft_above,y=price))+
  geom_point(color = "#5ab4ac")+
  
  stat_smooth(aes(x=sqft_above,y=price),method="lm", color="#e34a33")+
  theme_bw()+
  theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  xlab("Sqft Above")+
  ylab("Price")
```

As we could see, the distribution of this variable is very similar to sqft_living, thus we will go ahead a drop this variable. A more interesting variable to be considered in this case is floor, which indicates the number of floor in the house. 

#### Price & Floor 

Floors in the house is mezzanine, that's the reason it has values of 2.5 , 3.5. 

```{r}
ggplot(train,aes(factor(floors),log10(price),fill=factor(floors)))+ geom_boxplot(alpha=0.5)+ theme_bw() + 
  labs(x = "Floors", y = "Log Price")+ 
  theme(legend.position="none")
```

Median house price increases as bedroom increases up until 2.5. But then it start to decrease. Thus this might not be very indicative.

#### Price & Bedroom 

```{r}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% filter(bedrooms<30)%>%
ggplot(aes(x=bedrooms,y=price,col=bedrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bedrooms vs Price")+ scale_color_gradientn(colors=mycolors)+theme(legend.position="none")

```

As we can see, the majority of houses in the data set has 3 to 6 bed rooms. While there appear to be a positive trend, there is no clear correlation between the number of bedroom and house price. 

#### Price & Bathroom

```{r}
mycolors = c(brewer.pal(name="Dark2", n = 8), brewer.pal(name="Paired", n = 6))

train %>% 
ggplot(aes(x=bathrooms,y=price, col=bathrooms))+
geom_point(alpha=0.5,size=2)+
geom_smooth(method="lm",se=F,col="#e34a33")+
labs("title=Bathrooms vs Price")+theme(legend.position="none")
```

There is a clearer positive correlation between the number of bathroom and house price. Highest house price tend to have higher number of bathroom. This might be because house with more bathrooms mean there are more master bedrooms, thus increase the convenience and value of the house. It might be interesting to calculate the ratio of bathroom per bedroom for each house, this will be consider in the feature engineering part.

####Price & Grade 

This is the overall grade given to the housing unit, based on King County grading system. 

```{r}
ggplot(train,aes(grade,log10(price),fill=factor(grade)))+ geom_boxplot(alpha=0.5)+ theme_bw() + 
  labs(x = "Grade", y = "Log Price")+ 
  theme(legend.position="none")
```

As we could observe from the graph, the median house price increases as the grade increases. Higher quality house must have had higher price. 

#### Price & Location 

```{r}
df_map <- train

df_map$PriceBin<-cut(df_map$price, c(0,250e3,500e3,750e3,1e6,2e6,999e6))

center_lon = median(df_map$long,na.rm = TRUE)
center_lat = median(df_map$lat,na.rm = TRUE)

factpal <- colorFactor(c("#2166ac","#67a9cf","#ffffbf","#fee08b","#ef8a62","#b2182b"), 
                       df_map$PriceBin)

leaflet(df_map) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%
  addCircles(lng = ~long, lat = ~lat, 
             color = ~factpal(PriceBin))  %>%
  # controls
  setView(lng=center_lon, lat=center_lat,zoom = 12) %>%
  
  addLegend("bottomright", pal = factpal, values = ~PriceBin,
            title = "House Price Distribution",
            opacity = 1)


```

The map above shows the distribution of the house and the price tag in different bin group. 
House with the red has the highest price and lowest in blue dots. Most of the high price houses locate in Seattle center and in Bellevue or Medina neighborhoods. Houses with lower price are locate toward the South or a little bit further away from the center. This illustration shows that location can be a valuable variable to explain price. 


<hr /> 

### Build Base Model Using Linear Regression 

First we partition the labelled data into train and test set. 

```{r}
f_partition<-function(df,test_proportion=0.2, seed=NULL){
  
  if(!is.null(seed)) set.seed(seed)
  
  train_index<-sample(nrow(df), floor(nrow(df)*(1-test_proportion)), replace = FALSE)
  df_train<-df[train_index,]
  df_test<-df[-train_index,]
  
  return(list(train=df_train, test=df_test))
}

whole_data <- f_partition(df = train,
                        test_proportion = 0.2,
                        seed = 123)
```

We adopt Linear Regression as our base model. We apply the algorithm to our train dataset
```{r}
BaseModel = lm(price ~ ., whole_data$train )
summary(BaseModel) 
```

Now we will make prediction using the trained Linear Regression model. This model will be tested on unseen data. We then be able to compare the prediction with the actual value, thus assessing the prediction capability of the model. 

```{r}
BasePred <-predict(BaseModel,whole_data$test)
base_metric <- data.frame(cbind(actuals=whole_data$test$price, predicteds=BasePred))
r2_base <- 1 - sum((base_metric$actuals-base_metric$predicteds)^2)/sum((base_metric$actuals-mean(base_metric$actuals))^2)
r2_base
```

Our current model has the R Squared of 0.68. We will go ahead to perform feature engineering to improve the performance of the model. 

#### Correlation Analysis

```{r}
corresult = cor(train)

corrplot(corresult, type="upper")

```

##### Find attributes that are highly correlated (ideally > 0.5)
```{r}
highlyCorrelated <- findCorrelation(corresult, cutoff=0.5)
colnames(train[,highlyCorrelated])
```

We could see from the correlation graph, sqft_living, sqft_living15, sqft_above, grade, and bathrooms are highly correlated. In other words, these variables are explaining the same variations of the data point. Thus we might have to remove and manipulated these variables to avoid multicollinearity. Now lets explore some variables that have high correlation with house price. These include: sqft_living,sqft_above, bedrooms, bathrooms, grade, and location.

###  Feature Importance

```{r}
importance = varImp(BaseModel)
varImportance <- data.frame(Variables = row.names(importance[1]), 
                              Importance = round(importance$Overall,2))
rankImportance <- varImportance %>%
    mutate(Rank = paste0('#',dense_rank(desc(Importance))))

ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                             y = Importance)) +
    geom_bar(stat='identity',colour="white", fill = "#5ab4ac") +
    geom_text(aes(x = Variables, y = 1, label = Rank),
              hjust=0, vjust=.5, size = 4, colour = 'black',
              fontface = 'bold') +
    labs(x = 'Variables', title = 'Relative Variable Importance') +
    coord_flip() + 
    theme_bw()

```
From the table above, we could see the relative importance of the variables. Location, grade, year_built and size of the house are among the most importance variables to explain house price. Other less important variables are sqft_lot, sqft_above, data related to renovation. Thus I will go ahead and drop those variables that are correlated and have low importance. 

```{r}
train_refined <- train %>%select(-sqft_lot15, -sqft_above, -sqft_living15, -sqft_lot, -sqft_basement,
                                 floors, -zipcode)
```

### Feature Creation 

```{r}
# Create variable that show the ratio of bathroom per bedroom.
train_refined$bath_bed <- train_refined$bathrooms/train_refined$bedrooms

#Create a variable to show the age of the house. We assume that the house is sold in 2015. If the house has been renovated, the age will count from the year of renovation
train_refined$year_old <- 2015 - train_refined$yr_built
for  (row in 1:nrow(train_refined)){
  if (train_refined$yr_renovated[row] > 0){
  train_refined$year_old[row] <- train_refined$year_old[row] - (train_refined$yr_renovated[row] - train_refined$yr_built[row])
}
}

```

Since the house price varies across zipcode however stay constant within the zipcode, thus having a variable to identify the specific neighborhood might help. Also, zipcode is not a meaningful numeric variable by itself, I would like to transform it into categorical variables that the model can understand. I have attempted to add another dataset with specific neighborhood then merge with original by zip code. Then I one hot encode all the neighborhood. However, this for some reason drop the r2 level.
It seems like removing zipcode does help improve the model. 

```{r}
train_refined <- train %>%select( -zipcode)
```

### Outlier Treatment {.tabset .tabset-fade .tabset-pills}
We perform outlier detection and treatment for all numeric variables

#### Bedroom

```{r}
ggplot(data = data.frame(train_refined$bedrooms), aes(x = "", y = train_refined$bedrooms)) + geom_boxplot( fill="#e31a1c", alpha=0.5, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bedrooms") + theme_minimal() + theme(axis.title = element_blank())
train_refined <- train_refined %>% filter(bedrooms < 30)
```

There is clearly one significant outlier with 33 bedrooms, we will remove this outlier and only keep data point that is less than 30 rooms.  

#### Bathroom

```{r}
ggplot(data = data.frame(train_refined$bathrooms), aes(x = "", y = train_refined$bathrooms)) + geom_boxplot( fill="#fd8d3c", alpha=0.3, outlier.colour = '#252525', outlier.size = 2) + ggtitle("Boxplot of Bathrooms") + theme_minimal() + theme(axis.title = element_blank())
```

There is no significant outlier for Bathroom. 

#### Square Feet Living

```{r}
ggplot(data = data.frame(train_refined$sqft_living), aes(x = "", y = train_refined$sqft_living)) + geom_boxplot( fill="#fed976", alpha=0.3, outlier.colour = 'black', outlier.size = 2) + ggtitle("Boxplot of Square Feet Living") + theme_minimal() + theme(axis.title = element_blank())

train_refined <- train_refined %>% filter(sqft_living < 8000)
```

Most of the house in the train dataset are below 8000. We will go ahead and remove 8 houses above 8000 square meters.

### Standardization & Correcting Skewness
```{r}
#train_refined$sqft_living <- scale(train_refined$sqft_living)
#train_refined$bedrooms <- scale(train_refined$bedrooms)
#train_refined$bathrooms <- scale(train_refined$bathrooms)
#ggplot(data.frame(train_refined$bathrooms), aes(x = train_refined$bathrooms))  + geom_density(fill="#e34a33")

```
I tried to rescale the variable however there is so improvement made to the r2 score. Thus this step will not be taken. 

### PCA

### Defining Evaluation Metrics
```{r}
mae<-function(real, predicted){
  return(mean(abs(real-predicted)))
}
mape<-function(real,predicted){
  return(mean(abs((real-predicted)/real)))
}
rmse<-function(real,predicted){
  return(sqrt(mean((real-predicted)^2)))
}
r2 <- function(real,predicted){
  return(1-sum((real - predicted)^2)/sum((real - mean(real))^2))
}
```

### Random Forest {.tabset .tabset-fade .tabset-pills}

First partition the data into train and test sets
```{r}
final_data <- f_partition(df = train_refined,
                        test_proportion = 0.2,
                        seed = 123)
```

#### Ranger Model 
```{r}
library(ranger)
#Define target variable
formula<-as.formula(price~.)

rf_1<-ranger(formula, final_data$train)
test_rf1<-predict(rf_1, final_data$test)$predictions
r2(real=final_data$test$price, predicted = test_rf1)
```

#### Another Random Forest Model
```{r}
Modelrf<-randomForest(formula=formula, data=final_data$train, importance = TRUE)
final_data$test$pred<-predict(Modelrf, newdata = final_data$test, type='response')
rmse(real=final_data$test$price, predicted = RF_pred)
r2(real=final_data$test$price, predicted = final_data$test$pred)
```


#### Summary
```{r}
summary(Modelrf) 
```

#### Variable Importance
```{r}
varImpPlot(Modelrf,type=2)
```

#### Plotting Prediction
```{r}
ggplot(final_data$test,aes(x=price,y=pred))+geom_point(color="#5ab4ac")+geom_abline(color="#e34a33")
```

### XGBoost Model 
```{r}
xgb_0<-xgboost(booster='gbtree',
               data=as.matrix(final_data$train[, -1]),
               label=final_data$train$price,
               nrounds = 50,
               objective='reg:linear')
print(xgb_0)

test_xgb<-predict(xgb_0, newdata = as.matrix(final_data$test[, -1]), type='response')

r2(real=final_data$test$price, predicted = test_xgb)
```
